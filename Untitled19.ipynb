{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL9ReF/C2OWLbTslJvtK7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sathwika2202/NLP/blob/main/Untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "rJGD2HZiUXX0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "25ZrtBPSMIGi"
      },
      "outputs": [],
      "source": [
        "D1 = \"I am working as a professor\"\n",
        "D2 = \"I am working in SR University\"\n",
        "D3 = \"I did my phd in NITW\"\n",
        "D4 = \"I did my masters in OUCE\"\n",
        "D5 = \"I completed my bachelors in JNTUH\"\n",
        "D6 = \"I am teaching machine learning to students\"\n",
        "D7 = \"I am guiding research scholars in artificial intelligence\"\n",
        "D8 = \"I published research papers in international journals\"\n",
        "D9 = \"I attended conferences in Hyderabad and Delhi\"\n",
        "D10 = \"I am interested in natural language processing research\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uni Gram Counts"
      ],
      "metadata": {
        "id": "b5kHgwT2Ubq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# Combine the text from D1, D2, D3, D4, D5, D6, D7, D8, D9, D10\n",
        "combined_text = f\"{D1} {D2} {D3} {D4} {D5} {D6} {D7} {D8} {D9} {D10}\"\n",
        "\n",
        "# Tokenize the combined text into words and convert to lowercase\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Calculate unigram counts\n",
        "unigram_counts = collections.Counter(words)\n",
        "\n",
        "# Print the unigram counts\n",
        "print(\"Unigram Counts:\")\n",
        "for word, count in unigram_counts.most_common():\n",
        "    print(f\"{word}: {count}\")\n",
        "#Vocabulary size is length of unigrams\n",
        "V=len(unigram_counts)\n",
        "print(\"Vocabulary Size=\",V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoM6PxPkMQDM",
        "outputId": "adc77bc2-30ba-4e2b-f867-c5ad3197449a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Counts:\n",
            "i: 10\n",
            "in: 8\n",
            "am: 5\n",
            "my: 3\n",
            "research: 3\n",
            "working: 2\n",
            "did: 2\n",
            "as: 1\n",
            "a: 1\n",
            "professor: 1\n",
            "sr: 1\n",
            "university: 1\n",
            "phd: 1\n",
            "nitw: 1\n",
            "masters: 1\n",
            "ouce: 1\n",
            "completed: 1\n",
            "bachelors: 1\n",
            "jntuh: 1\n",
            "teaching: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "to: 1\n",
            "students: 1\n",
            "guiding: 1\n",
            "scholars: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "published: 1\n",
            "papers: 1\n",
            "international: 1\n",
            "journals: 1\n",
            "attended: 1\n",
            "conferences: 1\n",
            "hyderabad: 1\n",
            "and: 1\n",
            "delhi: 1\n",
            "interested: 1\n",
            "natural: 1\n",
            "language: 1\n",
            "processing: 1\n",
            "Vocabulary Size= 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bi-Gram Counts"
      ],
      "metadata": {
        "id": "QU5Ox87uUnBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "combined_text = f\"{D1} {D2} {D3} {D4} {D5} {D6} {D7} {D8} {D9} {D10}\"\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "bigram_counts = collections.Counter(bigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nBigram Counts:\")\n",
        "for bigram, count in bigram_counts.most_common():\n",
        "    print(f\"{bigram[0]} {bigram[1]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJt-XZRsM6rv",
        "outputId": "329c8e79-4fad-45b9-f483-9215611c786f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Counts:\n",
            "i am: 5\n",
            "am working: 2\n",
            "i did: 2\n",
            "did my: 2\n",
            "working as: 1\n",
            "as a: 1\n",
            "a professor: 1\n",
            "professor i: 1\n",
            "working in: 1\n",
            "in sr: 1\n",
            "sr university: 1\n",
            "university i: 1\n",
            "my phd: 1\n",
            "phd in: 1\n",
            "in nitw: 1\n",
            "nitw i: 1\n",
            "my masters: 1\n",
            "masters in: 1\n",
            "in ouce: 1\n",
            "ouce i: 1\n",
            "i completed: 1\n",
            "completed my: 1\n",
            "my bachelors: 1\n",
            "bachelors in: 1\n",
            "in jntuh: 1\n",
            "jntuh i: 1\n",
            "am teaching: 1\n",
            "teaching machine: 1\n",
            "machine learning: 1\n",
            "learning to: 1\n",
            "to students: 1\n",
            "students i: 1\n",
            "am guiding: 1\n",
            "guiding research: 1\n",
            "research scholars: 1\n",
            "scholars in: 1\n",
            "in artificial: 1\n",
            "artificial intelligence: 1\n",
            "intelligence i: 1\n",
            "i published: 1\n",
            "published research: 1\n",
            "research papers: 1\n",
            "papers in: 1\n",
            "in international: 1\n",
            "international journals: 1\n",
            "journals i: 1\n",
            "i attended: 1\n",
            "attended conferences: 1\n",
            "conferences in: 1\n",
            "in hyderabad: 1\n",
            "hyderabad and: 1\n",
            "and delhi: 1\n",
            "delhi i: 1\n",
            "am interested: 1\n",
            "interested in: 1\n",
            "in natural: 1\n",
            "natural language: 1\n",
            "language processing: 1\n",
            "processing research: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tri-Gram Counts"
      ],
      "metadata": {
        "id": "I7-3y3LmUrSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "combined_text = f\"{D1} {D2} {D3} {D4} {D5} {D6} {D7} {D8} {D9} {D10} \"\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "Trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    Trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "Trigrams_counts = collections.Counter(Trigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nTrigrams Counts:\")\n",
        "for Trigrams, count in Trigrams_counts.most_common():\n",
        "    print(f\"{Trigrams[0]} {Trigrams[1]} {Trigrams[2]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyuETk5bNVKI",
        "outputId": "31f54c05-77c5-47f9-ec49-f68b4af58e34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigrams Counts:\n",
            "i am working: 2\n",
            "i did my: 2\n",
            "am working as: 1\n",
            "working as a: 1\n",
            "as a professor: 1\n",
            "a professor i: 1\n",
            "professor i am: 1\n",
            "am working in: 1\n",
            "working in sr: 1\n",
            "in sr university: 1\n",
            "sr university i: 1\n",
            "university i did: 1\n",
            "did my phd: 1\n",
            "my phd in: 1\n",
            "phd in nitw: 1\n",
            "in nitw i: 1\n",
            "nitw i did: 1\n",
            "did my masters: 1\n",
            "my masters in: 1\n",
            "masters in ouce: 1\n",
            "in ouce i: 1\n",
            "ouce i completed: 1\n",
            "i completed my: 1\n",
            "completed my bachelors: 1\n",
            "my bachelors in: 1\n",
            "bachelors in jntuh: 1\n",
            "in jntuh i: 1\n",
            "jntuh i am: 1\n",
            "i am teaching: 1\n",
            "am teaching machine: 1\n",
            "teaching machine learning: 1\n",
            "machine learning to: 1\n",
            "learning to students: 1\n",
            "to students i: 1\n",
            "students i am: 1\n",
            "i am guiding: 1\n",
            "am guiding research: 1\n",
            "guiding research scholars: 1\n",
            "research scholars in: 1\n",
            "scholars in artificial: 1\n",
            "in artificial intelligence: 1\n",
            "artificial intelligence i: 1\n",
            "intelligence i published: 1\n",
            "i published research: 1\n",
            "published research papers: 1\n",
            "research papers in: 1\n",
            "papers in international: 1\n",
            "in international journals: 1\n",
            "international journals i: 1\n",
            "journals i attended: 1\n",
            "i attended conferences: 1\n",
            "attended conferences in: 1\n",
            "conferences in hyderabad: 1\n",
            "in hyderabad and: 1\n",
            "hyderabad and delhi: 1\n",
            "and delhi i: 1\n",
            "delhi i am: 1\n",
            "i am interested: 1\n",
            "am interested in: 1\n",
            "interested in natural: 1\n",
            "in natural language: 1\n",
            "natural language processing: 1\n",
            "language processing research: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Bi-Gram Counts\n"
      ],
      "metadata": {
        "id": "8on17S_pUvln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = bigram_count / last_word_unigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "        return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPIJmf45NtvM",
        "outputId": "6aa409c7-db86-4e59-a773-59fe88b9bdd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  working is  0.4\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of  phd is  0.3333333333333333\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of  am is  0.5\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Bi-Gram Model\n"
      ],
      "metadata": {
        "id": "t9A0VRnnU5Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqHH2Za_OIt-",
        "outputId": "b79be2ff-2122-4bb6-a285-24f0d21c51d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of  working is  0.4\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Tri-Gram Counts"
      ],
      "metadata": {
        "id": "Fl_z05zBU8HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = trigram_count / last_two_words_bigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "        return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEkdMIv1QTl0",
        "outputId": "9e3f96a6-6395-404b-b1cb-1f2877f9eeab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.5\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.5\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Tri-Gram Model"
      ],
      "metadata": {
        "id": "Hiwu4aA5VC_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6HW2888QvPs",
        "outputId": "0d557417-0fe9-490b-b9cb-098d87ce72ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of  working is  0.4\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Bi-Gram Counts with Laplace Smoothening\n"
      ],
      "metadata": {
        "id": "5OO9nn8fRDUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_Laplace(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+1) / (last_word_unigram_count+V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_Laplace(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_Laplace(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"artifical\"\n",
        "next_word3 = predict_next_word_bigram_Laplace(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_Laplace(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh0ZL_l7REfF",
        "outputId": "1c8306c5-9022-48e9-9f21-8a33d64c2cb5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.06521739130434782\n",
            "probability of teaching is  0.043478260869565216\n",
            "probability of guiding is  0.043478260869565216\n",
            "probability of interested is  0.043478260869565216\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.045454545454545456\n",
            "probability of masters is  0.045454545454545456\n",
            "probability of bachelors is  0.045454545454545456\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "Given sequence: 'artifical', predicted next word: 'No bigram found starting with 'artifical'.'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Laplace Smoothening based Bi-Gram Model"
      ],
      "metadata": {
        "id": "tN3H0NAeSNeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_Laplace(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC2HmgD6SJ9w",
        "outputId": "7cfd80e3-2537-477b-e0bc-144e26d8f750"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of working is  0.06521739130434782\n",
            "probability of teaching is  0.043478260869565216\n",
            "probability of guiding is  0.043478260869565216\n",
            "probability of interested is  0.043478260869565216\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Tri-Gram Counts based on laplace smoothening"
      ],
      "metadata": {
        "id": "q-soURvjSZ64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_Laplace(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+1) / (last_two_words_bigram_count+V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_Laplace(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_Laplace(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szEA_A4mSY0U",
        "outputId": "9eac1762-1ebf-4e38-b7d4-9ed9aa60c90f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.046511627906976744\n",
            "probability of  in is  0.046511627906976744\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.046511627906976744\n",
            "probability of  masters is  0.046511627906976744\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Laplace Smoothening based Tri-Gram Model"
      ],
      "metadata": {
        "id": "FpE20HYdSvIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_Laplace(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJHIUlgOSuum",
        "outputId": "1cb1e206-d02f-4a7e-bd69-1d3f82fac212"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am Working\n",
            "probability of  as is  0.046511627906976744\n",
            "probability of  in is  0.046511627906976744\n",
            "Given sequence: 'I am Working', predicted next word: 'as'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Bi-Gram Counts with Add - K Smoothening\n"
      ],
      "metadata": {
        "id": "XvVMPAyJS-U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_K(word_sequence, bigram_counts, unigram_counts, K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+K) / (last_word_unigram_count+K*V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_K(sequence1, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_K(sequence2, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_K(sequence3, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_K(sequence4, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZR6g3xOS9He",
        "outputId": "6e23adca-0e1a-4ebd-c595-4e25a9f80f51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.09803921568627451\n",
            "probability of teaching is  0.058823529411764705\n",
            "probability of guiding is  0.058823529411764705\n",
            "probability of interested is  0.058823529411764705\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.06382978723404255\n",
            "probability of masters is  0.06382978723404255\n",
            "probability of bachelors is  0.06382978723404255\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of am is  0.18032786885245902\n",
            "probability of did is  0.08196721311475409\n",
            "probability of completed is  0.04918032786885246\n",
            "probability of published is  0.04918032786885246\n",
            "probability of attended is  0.04918032786885246\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment of Add-K Smoothening based Bi-Gram Model"
      ],
      "metadata": {
        "id": "Lsy9wjlWTchr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_K(ip_text, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0yyac-4TY8l",
        "outputId": "e6a2bc99-ea47-4e29-eb85-e5dafa8bfb48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of working is  0.09803921568627451\n",
            "probability of teaching is  0.058823529411764705\n",
            "probability of guiding is  0.058823529411764705\n",
            "probability of interested is  0.058823529411764705\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Using Tri-Gram Counts with Add - K Smoothening\n"
      ],
      "metadata": {
        "id": "0dREOSdMTs91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_K(word_sequence, Trigrams_counts, bigram_counts,K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+K) / (last_two_words_bigram_count+K*V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_K(sequence1, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_K(sequence2, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQO0YQalTsYI",
        "outputId": "290df4ab-c0fd-4376-b8cc-986b6bf6bdc5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  as is  0.06666666666666667\n",
            "probability of  in is  0.06666666666666667\n",
            "Given sequence: 'I am working', predicted next word: 'as'\n",
            "probability of  phd is  0.06666666666666667\n",
            "probability of  masters is  0.06666666666666667\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Deployment of Add-K Smoothening based Tri-Gram Model"
      ],
      "metadata": {
        "id": "ZXOh4gNLUDxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_K(ip_text, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o1tTR_tUDN_",
        "outputId": "193ebeed-e5a0-4342-b40b-dc4ab5d535e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of  working is  0.09803921568627451\n",
            "probability of  teaching is  0.058823529411764705\n",
            "probability of  guiding is  0.058823529411764705\n",
            "probability of  interested is  0.058823529411764705\n",
            "Given sequence: 'I am', predicted next word: 'working'\n"
          ]
        }
      ]
    }
  ]
}